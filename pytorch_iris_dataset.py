# -*- coding: utf-8 -*-
"""pytorch-iris_dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XqL9DpJqciU62f-HYNTxN5OwxIEOfhGQ
"""

#connect to google Colab
from google.colab import drive
drive.mount('/content/drive')

#get the data from drive
import pandas as pd

data=pd.read_csv('/content/drive/MyDrive/Iris.csv')
data.head(1)

#we don't need the column Id so i will delete it
data.drop(columns=['Id'], inplace=True)

data.head(2)

#utliser le gpu if available and disply thier number and names if not use cpu

import torch

# Check if GPU is available
if torch.cuda.is_available():
    # Get the number of GPUs
    num_gpus = torch.cuda.device_count()
    # Get the names of the GPUs
    gpu_names = [torch.cuda.get_device_name(i) for i in range(num_gpus)]
    # Print the number and names of the GPUs
    print(f"Number of GPUs: {num_gpus}")
    print(f"GPU names: {gpu_names}")
    device="cuda"
else:
    device='cpu'

print(f"you are using {device} ")

#we need to convert the labels to numbers because machine learning don't deal with string ;)
#let's do it
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
data['Species']=le.fit_transform(data['Species'])

#separated our data into labels ans features
features=data.drop(columns=['Species']).values
labels=data['Species'].values

features[0],labels[0]
#it's cool is it?

#split our data into train and test
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)

"""we will use pytorch to build, train , test our model so we need to add our data to a dataloader and dataset to facilate our work don't worry it is simple"""

import torch
from torch.utils.data import DataLoader
from torch.utils.data import Dataset
from torchvision import datasets
class CustomDataset(Dataset):
    def __init__(self,features,labels):
      self.features=features
      self.labels=labels

    def __len__(self):
        return self.features.shape[0]
    def __getitem__(self, idx):
       f=self.features[idx]
       l=self.labels[idx]
       f=torch.tensor(f)

       l=torch.tensor(l)
       return f,l

train=CustomDataset(X_train,y_train)

test=CustomDataset(X_test,y_test)

train_data = DataLoader(train, batch_size=16, shuffle=True)
test_data = DataLoader(test, batch_size=16, shuffle=False)

print(len(test_data))

"""building our model"""

from torch import nn
class Iris(nn.Module):
  def __init__(self):
    super().__init__()
    self.layer1=nn.Linear(4,3)
    self.relu = nn.ReLU()
    self.layer2=nn.Linear(3,3)
  def forward(self,x):
    x=self.layer1(x)
    x=self.relu(x)
    x=self.layer2(x)

    return x
model1=Iris()

#convert the parameters of the model to a float64
for param in model1.parameters():
    param.data = param.data.to(torch.float64)

# creating our optimizer and loss function object
learning_rate = 0.01
loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model1.parameters(),lr=learning_rate)

epochs = 100
losses_per_epoch = []

for epoch in range(epochs):
    running_loss = 0.0
    for i, (x, y) in enumerate(train_data):
        # Forward pass
        y_predict = model1(x)

        # Compute the loss
        loss = loss_fn(y_predict, y)
        running_loss += loss.item()

        # Backward pass and optimization
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    # Compute the average loss for the epoch
    epoch_loss = running_loss / len(train_data)

    # Memorize the loss for the epoch
    losses_per_epoch.append(epoch_loss)

    # Print epoch and loss for the epoch
    print(f"epoch: {epoch}  loss: {epoch_loss}")

# Now you have the losses for each epoch stored in the list "losses_per_epoch"

import matplotlib.pyplot as plt
plt.plot(range(epochs),losses_per_epoch)
plt.ylabel('Loss')
plt.xlabel('epoch');

"""create a function for train and test"""

import torch.nn.functional as F

preds = []
correct = 0
total = 0

with torch.no_grad():
    for x, y in test_data:
        predict = model1(x)
        predict = F.softmax(predict, dim=1)  # Appliquer softmax
        preds.extend(predict.argmax(dim=1).tolist())

        # Comparer chaque prédiction avec l'étiquette correspondante
        correct += (predict.argmax(dim=1) == y).sum().item()
        total += y.size(0)  # Nombre total d'exemples dans ce batch

accuracy = correct / total
print(f"Accuracy: {accuracy}")